from src.text_suppliers.huffington_post import HuffingtonPostTextSupplier
from src.text_suppliers.textfile_supplier import TextFileSupplier
from src.tokenizer import Tokenizer


def test1():
    text_supplier = TextFileSupplier("src/tests/test1_input.txt")
    tokenizer = Tokenizer(text_supplier)
    tokenized_dict = tokenizer.tokenize()
    assert tokenized_dict == {"SOME": 1, "TEXT": 1, ".": 3, "AND": 1, "MORE": 1}


def test2():
    text_supplier = TextFileSupplier("src/tests/test2_input.txt")
    tokenizer = Tokenizer(text_supplier)
    tokenized_dict = tokenizer.tokenize()
    assert tokenized_dict == {
        "SOME": 1,
        "\n\n": 2,
        "\n": 1,
        "TEXT": 1,
        "ON": 1,
        "MULTIPLE": 1,
        "-": 1,
        "LINES": 1,
        "!": 1,
    }


def test3():
    text_supplier = HuffingtonPostTextSupplier(
        "https://www.huffingtonpost.co.uk/entry/french-newspaper-criticises-america-over-shooting_uk_628f4893e4b0b1d984520e76"
    )
    tokenizer = Tokenizer(text_supplier)
    tokenized_dict = tokenizer.tokenize()
    assert tokenized_dict == {
        "FRANCE’S": 1,
        "LE": 3,
        "MONDE": 3,
        "NEWSPAPER": 4,
        "DESCRIBED": 1,
        "THE": 18,
        "UNITED": 3,
        "STATES": 3,
        "AS": 1,
        "A": 4,
        "COUNTRY": 2,
        "“TRAPPED": 1,
        "IN": 6,
        "MADNESS”": 1,
        "AFTER": 2,
        "YET": 1,
        "ANOTHER": 2,
        "MASS": 1,
        "SHOOTING": 2,
        "AT": 1,
        "SCHOOL": 1,
        "CLAIMED": 1,
        "LIVES": 1,
        "OF": 6,
        "19": 1,
        "CHILDREN": 1,
        "AND": 5,
        "TWO": 1,
        "TEACHERS": 1,
        "TEXAS": 1,
        "EARLIER": 1,
        "THIS": 2,
        "WEEK": 1,
        ".": 10,
        "\n": 8,
        "CALLED": 1,
        "IT": 3,
        "CYCLE": 1,
        "DESPAIR": 1,
        "THAT": 3,
        "KEEPS": 1,
        "REPEATING": 1,
        "“THE": 1,
        "ONE": 2,
        "PARKLAND,": 1,
        "2018,": 1,
        "CHANGED": 1,
        "NOTHING": 1,
        "DESPITE": 1,
        "EXCEPTIONAL": 1,
        "ACTIVISM": 1,
        "STUDENTS": 1,
        "WHO": 1,
        "HAD": 1,
        "ESCAPED": 1,
        "THEY": 2,
        "BELIEVED": 1,
        "WAS": 1,
        "POSSIBLE": 1,
        "TO": 3,
        "BRING": 1,
        "SICK": 1,
        "ITS": 2,
        "VIOLENCE": 1,
        "BACK": 1,
        "SENSES": 1,
        "REMIND": 1,
        "ELECTED": 1,
        "OFFICIALS": 1,
        "THEIR": 1,
        "RESPONSIBILITIES,”": 1,
        "WROTE": 2,
        "“BUT": 1,
        "FAILED": 1,
        "”": 2,
        "THEN,": 1,
        "TURNED": 1,
        "CONCEPT": 1,
        "“AMERICAN": 1,
        "EXCEPTIONALISM”": 1,
        "―": 2,
        "NOTION": 1,
        "THERE": 2,
        "IS": 6,
        "SOMETHING": 1,
        "SPECIAL": 1,
        "UNIQUE": 1,
        "ABOUT": 1,
        "UPSIDE": 1,
        "DOWN": 1,
        "“IF": 1,
        "ANY": 1,
        "AMERICAN": 1,
        "EXCEPTIONALISM,": 1,
        "TOLERATE": 1,
        "FACT": 1,
        "SCHOOLS": 1,
        "ARE": 1,
        "REGULARLY": 1,
        "TRANSFORMED": 1,
        "INTO": 1,
        "BLOODY": 1,
        "RANGES,”": 1,
        "PLACED": 1,
        "BLAME": 1,
        "ON": 1,
        "GOP:": 1,
        "“INDEED,": 1,
        "AMERICA": 1,
        "KILLING": 1,
        "ITSELF": 1,
        "REPUBLICAN": 2,
        "PARTY": 1,
        "LOOKING": 1,
        "OTHER": 1,
        "WAY,": 1,
        "IDEOLOGICALLY": 1,
        "COMPLICIT": 1,
        "TRAGEDY": 1,
        "“MORE": 1,
        "MORE": 1,
        "GUNS:": 1,
        "ONLY": 1,
        "CREDO,”": 1,
        "EDITORIAL": 2,
        "ADDED": 1,
        "READ": 1,
        "FULL": 1,
        "HERE": 1,
    }
